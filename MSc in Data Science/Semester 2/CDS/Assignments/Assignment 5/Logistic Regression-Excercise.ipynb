{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6169205a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c87582e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import idx2numpy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e584b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "622481f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Features:(n_examples, n_features)\n",
    "#     Labels: (n_examples, 1)\n",
    "#     Weights:(n_features, 1)\n",
    "\n",
    "\n",
    "# -- Sigmoid function -- #\n",
    "def sigmoid(z):\n",
    "    \" Return the sigmoid function \"\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "# -- Predictor function -- #\n",
    "def predict(features, weights):\n",
    "    '''\n",
    "    Returns probabilities\n",
    "    of class label\n",
    "    '''\n",
    "    z = np.dot(features, weights)\n",
    "    return sigmoid(z)\n",
    "\n",
    "# -- Cost function -- #\n",
    "def cost_function(features, labels, weights):\n",
    "    '''    \n",
    "    Write down the average cross-entropy loss\n",
    "    '''\n",
    "    observations = len(labels)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    # You may use this format or define in your own way\n",
    "    # Note: This is for binary cros-entropy loss.\n",
    "    \n",
    "    # We can divide the binary cross entropy loss into two parts\n",
    "    # One for p(y=1) and another for p(y=0)\n",
    "\n",
    "    # Take the error when label=1\n",
    "    class1_cost = np.dot(labels, np.log(predictions)) \n",
    "\n",
    "    # Take the error when label=0\n",
    "    class2_cost = np.dot((1 - labels), np.log(1 - predictions))\n",
    "\n",
    "    # Take the sum of both costs\n",
    "    cost = np.sum(class1_cost,class2_cost)\n",
    "\n",
    "    # Take the average cost\n",
    "    cost = - (cost/observations)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def update_weights(features, labels, weights, lr):\n",
    "    '''\n",
    "   Gradient Descent\n",
    "    '''\n",
    "    N = len(features.columns)# number of features\n",
    "\n",
    "    #1 - Get Predictions\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    gradient = np.dot((predictions - labels) , features) # Write down the gradient of the cost\n",
    "\n",
    "    # 3 Take the average cost derivative for each feature\n",
    "    gradient /= N\n",
    "\n",
    "    # 4 Update the weights using the learning rate lr\n",
    "    weights = weights - lr * gradient # Write down the update rule here\n",
    "\n",
    "    return weights\n",
    "\n",
    "def decision_boundary(prob):\n",
    "    return 1 if prob >= .5 else 0 # You may change the threshold from 0.5 \n",
    "\n",
    "def train(features, labels, weights, lr, iters):\n",
    "    cost_history = [np.float64('inf')]\n",
    "\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        \n",
    "        cost_history.append(cost)\n",
    "\n",
    "        print(\"iter: \"+str(i) + \" cost: \"+str(cost))\n",
    "\n",
    "    return weights, cost_history\n",
    "\n",
    "def accuracy(predicted_labels, actual_labels):\n",
    "    diff = predicted_labels - actual_labels\n",
    "    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79473aa",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fe1a4db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.58878035, -1.83866169, -1.91523246, ..., -1.89015625,\n",
       "         -0.43791834, -3.02046427],\n",
       "        [-0.4544144 , -1.26805567,  1.56154775, ...,  2.73278536,\n",
       "         -2.99483166,  1.55683739],\n",
       "        [ 2.0700028 ,  0.87351957, -0.23769278, ..., -1.13280434,\n",
       "         -4.24606079, -1.40908083],\n",
       "        ...,\n",
       "        [-4.10247057,  2.25632609, -4.84383965, ...,  2.50806318,\n",
       "          0.28860853, -2.70506016],\n",
       "        [ 1.34312391, -0.68034037, -0.83950483, ...,  0.38360686,\n",
       "         -0.51662404,  0.22532075],\n",
       "        [ 1.87693361, -1.98231942,  2.22928456, ..., -4.32192539,\n",
       "         -0.72617635, -0.84724951]]),\n",
       " array([2, 6, 0, 9, 4, 8, 7, 7, 6, 6, 4, 8, 6, 2, 3, 4, 8, 6, 4, 3, 7, 4,\n",
       "        2, 4, 7, 7, 0, 5, 8, 4, 3, 0, 3, 5, 8, 4, 0, 7, 7, 8, 3, 6, 8, 1,\n",
       "        4, 9, 0, 3, 7, 0, 2, 9, 9, 0, 0, 1, 0, 3, 4, 2, 8, 5, 9, 1, 8, 2,\n",
       "        6, 6, 9, 1, 0, 2, 8, 4, 3, 7, 4, 0, 2, 8, 2, 3, 4, 2, 9, 5, 3, 7,\n",
       "        1, 7, 1, 0, 6, 2, 0, 7, 8, 1, 2, 9, 0, 7, 1, 5, 7, 6, 3, 1, 1, 2,\n",
       "        1, 3, 1, 5, 7, 1, 0, 0, 0, 0, 9, 6, 5, 2, 1, 7, 3, 4, 4, 6, 8, 5,\n",
       "        4, 1, 3, 9, 9, 2, 7, 5, 5, 2, 4, 5, 5, 4, 8, 0, 2, 8, 1, 7, 0, 4,\n",
       "        5, 8, 8, 1, 8, 7, 5, 4, 8, 9, 6, 2, 6, 9, 8, 6, 5, 4, 4, 4, 2, 6,\n",
       "        3, 5, 3, 4, 4, 1, 1, 9, 4, 0, 0, 7, 0, 6, 1, 1, 4, 1, 8, 5, 2, 7,\n",
       "        2, 7, 9, 1, 0, 6, 1, 5, 7, 7, 9, 9, 4, 5, 3, 1, 3, 2, 8, 6, 5, 9,\n",
       "        2, 6, 7, 5, 7, 9, 2, 2, 6, 4, 9, 4, 4, 4, 9, 5, 3, 4, 6, 7, 0, 4,\n",
       "        1, 1, 2, 8, 9, 3, 2, 1, 0, 9, 5, 5, 6, 8, 3, 0, 9, 2, 3, 5, 4, 3,\n",
       "        8, 6, 4, 9, 7, 8, 1, 0, 7, 7, 0, 7, 2, 3, 7, 2, 6, 9, 4, 1, 2, 5,\n",
       "        1, 1, 9, 2, 1, 2, 1, 6, 6, 8, 1, 5, 4, 0, 1, 9, 8, 2, 4, 2, 0, 1,\n",
       "        4, 6, 5, 3, 4, 0, 2, 0, 2, 0, 8, 4, 9, 3, 1, 7, 1, 2, 0, 3, 8, 5,\n",
       "        2, 9, 1, 8, 0, 1, 3, 4, 5, 0, 1, 8, 2, 3, 4, 0, 6, 3, 1, 0, 7, 1,\n",
       "        8, 8, 2, 5, 9, 6, 7, 7, 5, 8, 2, 4, 9, 0, 1, 3, 0, 6, 1, 5, 9, 6,\n",
       "        6, 2, 7, 6, 7, 6, 9, 6, 0, 3, 8, 4, 4, 9, 9, 5, 3, 6, 2, 5, 1, 4,\n",
       "        3, 7, 3, 6, 8, 3, 2, 0, 3, 2, 9, 0, 5, 5, 8, 6, 5, 8, 3, 9, 1, 1,\n",
       "        1, 8, 2, 8, 1, 4, 0, 3, 7, 4, 3, 6, 1, 0, 6, 8, 3, 3, 5, 6, 2, 3,\n",
       "        2, 4, 7, 7, 5, 1, 1, 4, 7, 8, 7, 2, 6, 7, 1, 4, 7, 3, 0, 5, 1, 4,\n",
       "        7, 0, 9, 2, 0, 1, 5, 1, 2, 2, 0, 3, 7, 7, 9, 8, 0, 7, 3, 6, 2, 8,\n",
       "        5, 3, 5, 2, 6, 6, 4, 9, 0, 4, 2, 1, 0, 8, 7, 3, 5, 1, 2, 5, 2, 0,\n",
       "        5, 6, 6, 3, 6, 5, 8, 6, 0, 1, 7, 2, 0, 3, 1, 0, 7, 1, 3, 1, 6, 8,\n",
       "        3, 9, 8, 5, 1, 3, 2, 5, 5, 3, 6, 7, 6, 8, 8, 1, 8, 1, 9, 7, 6, 4,\n",
       "        2, 8, 8, 5, 3, 3, 6, 3, 5, 1, 7, 6, 2, 5, 9, 7, 8, 0, 9, 9, 5, 8,\n",
       "        5, 0, 4, 9, 5, 7, 8, 6, 7, 1, 8, 0, 0, 3, 6, 8, 0, 3, 9, 7, 1, 3,\n",
       "        5, 1, 9, 9, 6, 8, 3, 3, 2, 5, 3, 8, 2, 7, 6, 0, 0, 9, 1, 4, 5, 0,\n",
       "        4, 3, 6, 8, 4, 8, 1, 8, 0, 7, 3, 9, 1, 5, 9, 4, 9, 2, 4, 9, 5, 0,\n",
       "        4, 3, 2, 3, 0, 8, 5, 8, 3, 2, 6, 8, 8, 8, 0, 6, 5, 9, 1, 4, 5, 6,\n",
       "        5, 8, 3, 6, 4, 3, 0, 7, 4, 2, 2, 5, 7, 9, 4, 6, 0, 4, 9, 7, 7, 9,\n",
       "        3, 2, 1, 7, 0, 2, 2, 3, 3, 4, 5, 0, 5, 1, 9, 3, 1, 2, 0, 3, 2, 8,\n",
       "        1, 5, 4, 9, 9, 7, 6, 1, 6, 4, 8, 3, 9, 8, 7, 6, 7, 4, 8, 4, 3, 3,\n",
       "        9, 9, 8, 6, 5, 2, 3, 8, 6, 1, 4, 5, 0, 0, 7, 5, 7, 3, 9, 3, 2, 7,\n",
       "        4, 5, 0, 0, 6, 5, 8, 1, 2, 7, 5, 2, 8, 6, 3, 0, 3, 9, 8, 9, 2, 1,\n",
       "        7, 8, 1, 1, 7, 9, 9, 4, 1, 5, 2, 3, 0, 1, 9, 5, 4, 9, 0, 6, 6, 7,\n",
       "        9, 9, 0, 4, 5, 3, 8, 3, 0, 7, 2, 0, 9, 7, 6, 2, 6, 5, 9, 8, 6, 8,\n",
       "        6, 1, 9, 6, 2, 5, 1, 2, 8, 2, 3, 2, 7, 4, 3, 4, 8, 9, 7, 2, 7, 5,\n",
       "        7, 5, 2, 4, 7, 0, 9, 6, 1, 1, 3, 8, 2, 3, 6, 3, 3, 5, 6, 2, 8, 1,\n",
       "        6, 5, 7, 9, 0, 2, 3, 5, 0, 6, 9, 8, 1, 8, 4, 6, 4, 6, 3, 8, 0, 1,\n",
       "        6, 2, 5, 5, 1, 1, 2, 8, 3, 9, 7, 8, 5, 0, 1, 7, 3, 7, 7, 0, 0, 9,\n",
       "        0, 9, 1, 6, 2, 4, 8, 0, 2, 9, 2, 9, 4, 5, 2, 8, 0, 1, 4, 5, 7, 0,\n",
       "        9, 5, 3, 2, 9, 7, 8, 1, 4, 1, 4, 5, 8, 2, 1, 2, 4, 4, 3, 7, 6, 6,\n",
       "        5, 8, 0, 8, 7, 3, 2, 6, 8, 6, 9, 9, 4, 9, 9, 9, 9, 1, 7, 4, 7, 6,\n",
       "        5, 5, 0, 7, 7, 0, 9, 0, 8, 5, 6, 7, 7, 5, 4, 4, 4, 6, 4, 0, 9, 6,\n",
       "        5, 7, 7, 6, 6, 5, 9, 4, 2, 9]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data set here\n",
    "data = sklearn.datasets.make_classification(n_samples=1000, n_classes=10,n_clusters_per_class=1, n_features= 10,n_informative=10, n_redundant=0, n_repeated=0) \n",
    "\n",
    "data # print a snippet of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae0836ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data[0])# Features\n",
    "y = data[1]# Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dc1aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76b8b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in np.unique(y_train) :\n",
    "    l1 = np.zeros(len(y_train))\n",
    "    l1[np.where(y_train == i)] = 1\n",
    "    train_data.append(l1)\n",
    "    \n",
    "    \n",
    "###################\n",
    "\n",
    "test_data = []\n",
    "for i in np.unique(y_test) :\n",
    "    l1 = np.zeros(len(y_test))\n",
    "    l1[np.where(y_test == i)] = 1\n",
    "    test_data.append(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc19bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de81dcaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14152/3428130956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0moptimal_wts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_wts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14152/497289322.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(features, labels, weights, lr, iters)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m#Calculate error for auditing purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mcost_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14152/497289322.py\u001b[0m in \u001b[0;36mcost_function\u001b[1;34m(features, labels, weights)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Take the sum of both costs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass1_cost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass2_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Take the average cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2245\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2247\u001b[1;33m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[0;32m   2248\u001b[0m                           initial=initial, where=where)\n\u001b[0;32m   2249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     45\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0;32m     46\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Initialize the weights\n",
    "weights = []\n",
    "losses = []\n",
    "for i in train_data :\n",
    "    wts = np.random.normal(0, 1, len(x_train.columns))\n",
    "    # Training\n",
    "    optimal_wts, loss = train(x_train, i, wts, 1e-5, 1000)\n",
    "    weights.append(optimal_wts)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss against the number of epochs\n",
    "plt.plot(np.arange(1, len(loss)+1, 1), loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for elem in predict(X, optimal_wts):\n",
    "    predictions.append(decision_boundary(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the decision boundary for two features. For that, we will assign a color to each\n",
    "# # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "# x_min, x_max = X['Feature_1'].min() - 0.5, X['Feature_1'].max() + 0.5\n",
    "# y_min, y_max = X['Feature_2'].min() - 0.5, X['Feature_2'].max() + 0.5\n",
    "# h = 0.02  # step size in the mesh\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# probs = predict(np.c_[xx.ravel(), yy.ravel()], optimal_wts)\n",
    "# mesh_preds = []\n",
    "# for elem in probs:\n",
    "#     mesh_preds.append(decision_boundary(elem))\n",
    "# Z = np.array(mesh_preds)\n",
    "# # Put the result into a color plot\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure(1, figsize=(12, 5))\n",
    "# plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# # Plot also the training points\n",
    "# plt.scatter(X['Feature_1'], X['Feature_2'], c=y, edgecolors=\"k\", cmap=plt.cm.Paired)\n",
    "# plt.xlabel(\"Feature_1\")\n",
    "# plt.ylabel(\"Feature_2\")\n",
    "# plt.title('Gradient Descent')\n",
    "# plt.xlim(xx.min(), xx.max())\n",
    "# plt.ylim(yy.min(), yy.max())\n",
    "# #plt.xticks(())\n",
    "# #plt.yticks(())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59466387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy using gradient descent: {}\".format('Write the accuracy function wrt predictions and true labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830663c",
   "metadata": {},
   "source": [
    "## Scikit-Learn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8783f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS\n",
    "clf_lbfgs = LogisticRegression(solver=).fit(X, y)\n",
    "\n",
    "# Newton-CG\n",
    "clf_newt = LogisticRegression(solver=).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b734023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the decision boundary. For that, we will assign a color to each\n",
    "# # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "# x_min, x_max = X['Feature_1'].min() - 0.5, X['Feature_1'].max() + 0.5\n",
    "# y_min, y_max = X['Feature_2'].min() - 0.5, X['Feature_2'].max() + 0.5\n",
    "# h = 0.02  # step size in the mesh\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Z_lbfgs = clf_lbfgs.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Z_newt = clf_newt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# # Put the result into a color plot\n",
    "# Z_lbfgs = Z_lbfgs.reshape(xx.shape)\n",
    "# Z_newt = Z_newt.reshape(xx.shape)\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "# ax1 = plt.subplot(2, 1, 1)\n",
    "# ax1.pcolormesh(xx, yy, Z_lbfgs, cmap=plt.cm.Paired)\n",
    "\n",
    "# # Plot also the training points\n",
    "# ax1.scatter(X['Feature_1], X['Feature_2'], c=y, edgecolors=\"k\", cmap=plt.cm.Paired)\n",
    "# ax1.set_xlabel(\"Feature_1\")\n",
    "# ax1.set_ylabel(\"Feature_2\")\n",
    "\n",
    "# ax1.set_xlim(xx.min(), xx.max())\n",
    "# ax1.set_ylim(yy.min(), yy.max())\n",
    "# ax1.set_title('L-BFGS')\n",
    "# #ax1.set_xticks(())\n",
    "# #ax1.set_yticks(())\n",
    "\n",
    "# ax2 = plt.subplot(2, 1, 2)\n",
    "# ax2.pcolormesh(xx, yy, Z_newt, cmap=plt.cm.Paired)\n",
    "\n",
    "# # Plot also the training points\n",
    "# ax2.scatter(X['Feature_1'], X['Feature_2'], c=y, edgecolors=\"k\", cmap=plt.cm.Paired)\n",
    "# ax2.set_xlabel(\"Feature_1\")\n",
    "# ax2.set_ylabel(\"Feature_2\")\n",
    "\n",
    "# ax2.set_xlim(xx.min(), xx.max())\n",
    "# ax2.set_ylim(yy.min(), yy.max())\n",
    "# ax2.set_title('Newton-CG')\n",
    "# #ax1.set_xticks(())\n",
    "# #ax1.set_yticks(())\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy using L-BFGS: {}\".format('Write the accuracy function wrt predictions and true labels'))\n",
    "print('\\n')\n",
    "print(\"Accuracy using Newton-CG: {}\".format('Write the accuracy function wrt predictions and true labels'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
