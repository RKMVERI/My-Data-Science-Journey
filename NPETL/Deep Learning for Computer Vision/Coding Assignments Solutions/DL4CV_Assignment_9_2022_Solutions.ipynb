{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df322c8a4f0d43aeab8be3871891aa7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cbf90541a3d49e780e055957fd0f778",
              "IPY_MODEL_c9486654c0614f45802f6c8138bb2535",
              "IPY_MODEL_5fb417f99d2f40579dff382572412b59"
            ],
            "layout": "IPY_MODEL_c41bb4aec01a437189a0c11064468207"
          }
        },
        "1cbf90541a3d49e780e055957fd0f778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_293ebceeebcf4ee7bcd8b16d99724901",
            "placeholder": "​",
            "style": "IPY_MODEL_f8c27265dec44e81a8155c34c4f8fb12",
            "value": "100%"
          }
        },
        "c9486654c0614f45802f6c8138bb2535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6adc035f9f454ce88cc3e085bf184fa0",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d087136dcc754c338b9eb25002581e60",
            "value": 169001437
          }
        },
        "5fb417f99d2f40579dff382572412b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd0ce73c0b04a1c9e3f0818d3d666e1",
            "placeholder": "​",
            "style": "IPY_MODEL_ca385695ca6742399acd3412d342bfcd",
            "value": " 169001437/169001437 [00:06&lt;00:00, 30453658.82it/s]"
          }
        },
        "c41bb4aec01a437189a0c11064468207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "293ebceeebcf4ee7bcd8b16d99724901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c27265dec44e81a8155c34c4f8fb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6adc035f9f454ce88cc3e085bf184fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d087136dcc754c338b9eb25002581e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fd0ce73c0b04a1c9e3f0818d3d666e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca385695ca6742399acd3412d342bfcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxQ-yMVz9tJG"
      },
      "outputs": [],
      "source": [
        "#imports: do not change them\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import argparse\n",
        "import pickle \n",
        "import os\n",
        "import torch.nn as nn \n",
        "from PIL import Image\n",
        "from torchvision import transforms \n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torchvision\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import random_split\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "import torchvision.utils as utils\n",
        "\n",
        "## Please DONOT remove these lines. \n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: MultiHead-Attention\n",
        "Given a set of query, key, value vectors, calculate the attention value using multihead attention.\n",
        "\n",
        "choose the closest option to the maximum value of the attention vector\n",
        "\n",
        "\n",
        "1.   0.5590\n",
        "2.   0.7851\n",
        "3.   0.2312\n",
        "4.   0.9452\n",
        "\n",
        "Answer (1)"
      ],
      "metadata": {
        "id": "_kvz_VZr3Otj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Please DO NOT change these values.\n",
        "#This is the config that will be used for implementing the Multi-head attention \n",
        "config = {\n",
        "    'QI_DIM': 64,\n",
        "    'VI_DIM': 64,\n",
        "    'KI_DIM': 64,\n",
        "    'QO_DIM': 32,\n",
        "    'VO_DIM': 32,\n",
        "    'OP_DIM': 32,\n",
        "    'NUM_HEADS': 8,\n",
        "}"
      ],
      "metadata": {
        "id": "g9mwliHI_1BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A class for implementing the Multi-head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "        #set the op dimension, number of heads based on the value given in config\n",
        "        self.op_dim = config['OP_DIM']\n",
        "        self.n_heads = config['NUM_HEADS']\n",
        "\n",
        "        #set the query dimension as QO_DIM from the config and value dimension as VO_DIM\n",
        "        self.query_dim = config['QO_DIM']\n",
        "        self.value_dim = config['VO_DIM']\n",
        "\n",
        "        #set the respective dimensions\n",
        "        self.QTrans = nn.Linear(config['QI_DIM'], config['QO_DIM'])\n",
        "        self.KTrans = nn.Linear(config['KI_DIM'], config['QO_DIM'])\n",
        "        self.VTrans = nn.Linear(config['VI_DIM'], config['VO_DIM'])\n",
        "        self.OTrans = nn.Linear(config['VO_DIM'], config['OP_DIM'])\n",
        "\n",
        "        #set the scaling factor as follows:\n",
        "        #scale = sqrt(QO_DIM/NUM_HEADS)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.query_dim // self.n_heads]))\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    \n",
        "    \n",
        "    def forward(self, q_inp, k_inp, v_inp, ret_atn=False):\n",
        "\n",
        "        #making sure that shapes are similar across dim=0\n",
        "        assert q_inp.shape[0] == k_inp.shape[0]\n",
        "\n",
        "        #setting batch size\n",
        "        batch_size = q_inp.shape[0]\n",
        "        seq_length = q_inp.shape[1]\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "        #pass q_inp, k_inp, v_inp through the linear layers defined in the\n",
        "        #init function\n",
        "        \n",
        "        Q = self.QTrans(q_inp)\n",
        "        K = self.KTrans(k_inp)\n",
        "        V = self.VTrans(v_inp)\n",
        "\n",
        "        #reshape the Q, K, V tensors to (batch_size, num_heads, -1, self.query_dim/num_heads)\n",
        "        #hint: you can also do the same by the view() of pytorch\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.query_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.query_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.value_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        \n",
        "\n",
        "        # define energy as matrix product of Q and K divided by self.scale value\n",
        "        #hint: use torch.matmul() for this and do not forget to reshape K vector as (batch_size, num_heads, self.query_dim/num_heads, -1)\n",
        "        #this should be done in order to carry out the matrix multiplication\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        #define attention as softmax of energy vector across the last dimension\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        #multiplying the attention with value tensor\n",
        "        attended = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        #passing the attended tensor through OTrans linear layer\n",
        "        out = self.OTrans(attended.view(batch_size, seq_length, -1))\n",
        "        \n",
        "        if ret_atn:\n",
        "            return out, attention\n",
        "        \n",
        "        else:\n",
        "            return out"
      ],
      "metadata": {
        "id": "7S-NdiLS-1Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Please DONOT remove these lines. \n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "\n",
        "q = torch.rand(4, 2, 64)\n",
        "k = torch.rand(4, 2, 64)\n",
        "v = torch.rand(4, 2, 64)\n",
        "## Please DONOT remove these lines. \n",
        "\n",
        "### YOUR CODE STARTS HERE ###\n",
        "\n",
        "#initialise an instance of MultiHeadAttention class\n",
        "d = MultiHeadAttention(config)\n",
        "\n",
        "#pass q, k, v values through the instance\n",
        "x, y = d(q, k, v, ret_atn = True)\n",
        "\n",
        "#print the max value of the attention and report your answer\n",
        "torch.max(y)\n",
        "\n",
        "### YOUR CODE ENDS HERE ###\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7oFAgh1AiLY",
        "outputId": "c4a6d3df-6c4f-4037-b366-69edce0e7cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5590, grad_fn=<MaxBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: Learning to pay attention\n",
        "For questions 2 & 3, we will implement the paper \"Learning to pay Attention\", published at ICLR2018. The paper focuses on Trainable Soft Visual Attention in CNNs for image classification task. Follow the steps given in the upcoming code blocks to answer question 2.\n",
        "\n",
        "What is the range of the mean value obtained?\n",
        "\n",
        "\n",
        "1.   0.01 - 0.1\n",
        "2.   0.1 - 0.3\n",
        "3.   0.5 - 0.8\n",
        "4.   1 - 1.5\n",
        "\n",
        "Answer (2)"
      ],
      "metadata": {
        "id": "akByJVIk9zC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a block that has conv->batchnorm->relu\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_conv, pool=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        features = [in_features] + [out_features for i in range(num_conv)]\n",
        "\n",
        "        #appending all the operations in the list and then converting them to sequential set of operations\n",
        "        layers = []\n",
        "        for i in range(len(features)-1):\n",
        "            layers.append(nn.Conv2d(in_channels=features[i], out_channels=features[i+1], kernel_size=3, padding=1, bias=True))\n",
        "            layers.append(nn.BatchNorm2d(num_features=features[i+1], affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU())\n",
        "            if pool:\n",
        "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "        self.op = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "#defining a projector block which is nothing but a conv layer that is used to transform the channel dimension\n",
        "class ProjectorBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ProjectorBlock, self).__init__()\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features, kernel_size=1, padding=0, bias=False)\n",
        "    def forward(self, inputs):\n",
        "        return self.op(inputs)\n",
        "\n",
        "\n",
        "#defining a linear attention block as mentioned in the paper\n",
        "class LinearAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_features, normalize_attn=True):\n",
        "        super(LinearAttentionBlock, self).__init__()\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "        #set self.mormalize_attn as the normalize_attn argument\n",
        "        self.normalize_attn = normalize_attn\n",
        "\n",
        "        #define self.op as a conv layer with in_channels=in_features, out_channels=1, kernel_size=1, padding=0 and with bias=False\n",
        "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1, kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "\n",
        "    def forward(self, l, g):\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "        #set N, C, W, H using the size of the 'l' tensor\n",
        "        #where N = number of batches, C = channels, H = height of the tensor, W = width of the tensor\n",
        "        N, C, W, H = l.size()\n",
        "\n",
        "        #add l and g as mentioned in the paper and pass it through the self.op layer\n",
        "        c = self.op(l+g) # batch_sizex1xWxH\n",
        "\n",
        "        #pass the output of the self.op layer through sigmoid activation function\n",
        "        a = torch.sigmoid(c)\n",
        "\n",
        "        #make the size of a same as l\n",
        "        #hint: you can do this by the tensor.expand_as() method of PyTorch\n",
        "\n",
        "        #multiple a and l using torch.mul()\n",
        "        g = torch.mul(a.expand_as(l), l)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        #using adaptive avg pooling as the final operation\n",
        "        g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
        "\n",
        "        return c.view(N,1,W,H), g\n"
      ],
      "metadata": {
        "id": "7mSxSxjnAlwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Please DONOT remove/change these lines. \n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "att = LinearAttentionBlock(in_features=16, normalize_attn=True)\n",
        "\n",
        "l = torch.rand(4, 16, 64, 64)\n",
        "g = torch.rand(4, 16, 64, 64)\n",
        "## Please DONOT remove/change these lines. \n",
        "\n",
        "\n",
        "### YOUR CODE STARTS HERE ###\n",
        "#pass the l and g vectors through the attention instance\n",
        "d, f = att(l, g)\n",
        "#take the mean of all the elements present in the output 'g' vector and mark your answer for question 2\n",
        "#hint: use torch.mean()\n",
        "val = torch.mean(f)\n",
        "print(val)\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOpojpEbns-B",
        "outputId": "d0e7b730-6cb4-4e90-f5fc-dbd13d0ee4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1692, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3: Training the Attention VGGNet\n",
        "Follow the steps in the below given code blocks to build and train the Attention VGGNet model on CIFAR100 dataset.\n",
        "\n",
        "What is the range of the test set accuracy obtained after training the model for 5 epochs?\n",
        "\n",
        "\n",
        "1.   0 - 2%\n",
        "2.   2 - 10%\n",
        "3.   40 - 50%\n",
        "4.   25 - 30%\n",
        "\n",
        "Answer (2)"
      ],
      "metadata": {
        "id": "xGq-e0tW_Cyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#building a VGGNet integrated with the attention implemented in the above code block\n",
        "class AttnVGG_before(nn.Module):\n",
        "    def __init__(self, im_size, num_classes, attention=True, normalize_attn=True):\n",
        "        super(AttnVGG_before, self).__init__()\n",
        "        self.attention = attention\n",
        "        # defining conv blocks with certain choices of input and output features\n",
        "        #this set of choices is taking from the VGG style model\n",
        "        self.conv_block1 = ConvBlock(3, 64, 2)\n",
        "        self.conv_block2 = ConvBlock(64, 128, 2)\n",
        "        self.conv_block3 = ConvBlock(128, 256, 3)\n",
        "        self.conv_block4 = ConvBlock(256, 512, 3)\n",
        "        self.conv_block5 = ConvBlock(512, 512, 3)\n",
        "        self.conv_block6 = ConvBlock(512, 512, 2, pool=True)\n",
        "        self.dense = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=int(im_size/32), padding=0, bias=True)\n",
        "\n",
        "        # defining the projector layers and the multiple attention blocks as mentioned in the paper\n",
        "        if self.attention:\n",
        "            self.projector = ProjectorBlock(256, 512)\n",
        "            self.attn1 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
        "            self.attn2 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
        "            self.attn3 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
        "\n",
        "        # final classification layer\n",
        "        if self.attention:\n",
        "            self.classify = nn.Linear(in_features=512*3, out_features=num_classes, bias=True)\n",
        "        else:\n",
        "            self.classify = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "        # passing the input through all the defined layers\n",
        "        #pass x through conv_block1 and collect the output in variable name x\n",
        "        x = self.conv_block1(x)\n",
        "\n",
        "        #pass x through conv_block2 and collect the output in variable name x\n",
        "        x = self.conv_block2(x)\n",
        "\n",
        "        #pass x through conv_block3 and collect the output in variable name l1\n",
        "        l1 = self.conv_block3(x) \n",
        "\n",
        "        #pass l1 through max pool 2d and collect the output in variable name x\n",
        "        x = F.max_pool2d(l1, kernel_size=2, stride=2, padding=0) # /2\n",
        "\n",
        "        #pass x through conv_block4 and collect the output in av ariable called l2\n",
        "        l2 = self.conv_block4(x) # /2\n",
        "\n",
        "        #pass l2 through max pool 2d and collect the output in a variable called x\n",
        "        x = F.max_pool2d(l2, kernel_size=2, stride=2, padding=0) # /4\n",
        "\n",
        "        #pass x through conv_block5 and collect the output in a variable called l3\n",
        "        l3 = self.conv_block5(x) # /4\n",
        "\n",
        "       #pass l3 through max pool 2d and collect the output in variable name x\n",
        "        x = F.max_pool2d(l3, kernel_size=2, stride=2, padding=0) # /8\n",
        "\n",
        "        #pass x through conv_block6 and collect the output in a variable called x\n",
        "        x = self.conv_block6(x) # /32\n",
        "\n",
        "        #pass x through self.dense and collect the output in a variable called g\n",
        "        g = self.dense(x) # batch_sizex512x1x1\n",
        "\n",
        "        # attention calculations are carried out according to the paper\n",
        "        if self.attention:\n",
        "\n",
        "            #pass l1 through self.projector and collect the output in variable t\n",
        "            #pass g and t through self.attn1 and collect the output in variable names c1 and g1 respectively\n",
        "            c1, g1 = self.attn1(self.projector(l1), g)\n",
        "\n",
        "            #pass l2 and g through self.attn2 and collect the output in variable names c2 and g2 respectively\n",
        "            c2, g2 = self.attn2(l2, g)\n",
        "\n",
        "            #pass l3 and g through self.attn3 and collect the output in variable names c3 and g3 respectively\n",
        "            c3, g3 = self.attn3(l3, g)\n",
        "\n",
        "            #concat g1, g2 and g3 across dim=1 and collect the output in a variable called g\n",
        "            #hint:use torch.cat()\n",
        "            g = torch.cat((g1,g2,g3), dim=1)\n",
        "\n",
        "            # pass g through self.classify layer and collect the output in a variable called x\n",
        "            x = self.classify(g)\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        #if attention is set to false\n",
        "        else:\n",
        "            c1, c2, c3 = None, None, None\n",
        "            x = self.classify(torch.squeeze(g))\n",
        "\n",
        "        return [x, c1, c2, c3]"
      ],
      "metadata": {
        "id": "1TejCUDeAiXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining arg parse and running it\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size\")\n",
        "parser.add_argument(\"--epochs\", type=int, default=5, help=\"number of epochs\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.1, help=\"initial learning rate\")\n",
        "parser.add_argument(\"-f\", \"--file\", required=False) \n",
        "\n",
        "\n",
        "opt = parser.parse_args()"
      ],
      "metadata": {
        "id": "Ozg7UV1B4qqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the main function for training\n",
        "def main():\n",
        "    print('\\nloading the dataset ...\\n')\n",
        "\n",
        "    #setting the image size to 32 as we will be using CIFAR100 dataset\n",
        "    im_size = 32\n",
        "\n",
        "    #defining train transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "    \n",
        "    #defining the trainset from the torchvision library\n",
        "    trainset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=True, download=True, transform=transform_train)\n",
        "    \n",
        "    #we will be training on a random 5000 image subset of the training data to save time\n",
        "    sub_train_size = 5000\n",
        "    train_size = len(trainset) - sub_train_size\n",
        "    extra_ds, train_ds = random_split(trainset, [train_size, sub_train_size], generator=torch.Generator().manual_seed(30))\n",
        "\n",
        "    #defining the dataloader\n",
        "    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=opt.batch_size, shuffle=False, num_workers=0, worker_init_fn = lambda id: np.random.seed(id))\n",
        "    # testset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=False, download=False, transform=transform_test)\n",
        "    # testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0, worker_init_fn = lambda id: np.random.seed(id))\n",
        "    print('done')\n",
        "\n",
        "    print('\\npay attention before maxpooling layers...\\n')\n",
        "    #calling the model that needs to be trained\n",
        "    net = AttnVGG_before(im_size=im_size, num_classes=100,\n",
        "        attention=True, normalize_attn=True)\n",
        "    \n",
        "    #setting the loss function to CE loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print('done')\n",
        "\n",
        "    #setting the device to GPU and transfering model and loss function tot he device\n",
        "    print('\\nmoving to GPU ...\\n')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = net.to(device)\n",
        "    criterion.to(device)\n",
        "    print('done')\n",
        "\n",
        "    # defining the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=opt.lr, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "    # training loop starts here\n",
        "    print('\\nstart training ...\\n')\n",
        "    step = 0\n",
        "\n",
        "    #looping over the total number of epochs\n",
        "    for epoch in range(opt.epochs):\n",
        "        \n",
        "        # adjusting the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "        # looping for every epoch\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            model.train()\n",
        "            model.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = data\n",
        "\n",
        "            #transfering the data to device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # forward passing the inputs to the model\n",
        "            pred, __, __, __ = model(inputs)\n",
        "\n",
        "            # backwardpass for loss calculation\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # display results for every 3 iterations\n",
        "            if i % 3 == 0:\n",
        "                model.eval()\n",
        "                pred, __, __, __ = model(inputs)\n",
        "                predict = torch.argmax(pred, 1)\n",
        "                total = labels.size(0)\n",
        "                correct = torch.eq(predict, labels).sum().double().item()\n",
        "                accuracy = correct / total\n",
        "                print(\"[epoch %d][%d/%d] loss %.4f accuracy %.2f%%\"\n",
        "                    % (epoch, i, len(trainloader)-1, loss.item(), (100*accuracy)))\n",
        "            step += 1\n",
        "\n",
        "    #saving the weights of the model at the end of total number of epochs\n",
        "    torch.save(model.state_dict(), 'net.pth')"
      ],
      "metadata": {
        "id": "6G_4laZt3vph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#please change the runtime type to GPU before running this cell\n",
        "#follow the instructions given in this blog for more details: https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "df322c8a4f0d43aeab8be3871891aa7b",
            "1cbf90541a3d49e780e055957fd0f778",
            "c9486654c0614f45802f6c8138bb2535",
            "5fb417f99d2f40579dff382572412b59",
            "c41bb4aec01a437189a0c11064468207",
            "293ebceeebcf4ee7bcd8b16d99724901",
            "f8c27265dec44e81a8155c34c4f8fb12",
            "6adc035f9f454ce88cc3e085bf184fa0",
            "d087136dcc754c338b9eb25002581e60",
            "3fd0ce73c0b04a1c9e3f0818d3d666e1",
            "ca385695ca6742399acd3412d342bfcd"
          ]
        },
        "id": "-xz929PM7Kan",
        "outputId": "31418ca5-88a7-4643-abde-1ebf92dc5071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loading the dataset ...\n",
            "\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to CIFAR100_data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df322c8a4f0d43aeab8be3871891aa7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting CIFAR100_data/cifar-100-python.tar.gz to CIFAR100_data\n",
            "done\n",
            "\n",
            "pay attention before maxpooling layers...\n",
            "\n",
            "done\n",
            "\n",
            "moving to GPU ...\n",
            "\n",
            "done\n",
            "\n",
            "start training ...\n",
            "\n",
            "\n",
            "epoch 0 learning rate 0.095000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 0][0/39] loss 4.6173 accuracy 1.56%\n",
            "[epoch 0][3/39] loss 4.6021 accuracy 1.56%\n",
            "[epoch 0][6/39] loss 4.5919 accuracy 0.78%\n",
            "[epoch 0][9/39] loss 4.5264 accuracy 0.00%\n",
            "[epoch 0][12/39] loss 4.4593 accuracy 0.78%\n",
            "[epoch 0][15/39] loss 4.5684 accuracy 4.69%\n",
            "[epoch 0][18/39] loss 4.5032 accuracy 3.91%\n",
            "[epoch 0][21/39] loss 4.3271 accuracy 4.69%\n",
            "[epoch 0][24/39] loss 4.2961 accuracy 3.91%\n",
            "[epoch 0][27/39] loss 4.4167 accuracy 0.00%\n",
            "[epoch 0][30/39] loss 4.5069 accuracy 2.34%\n",
            "[epoch 0][33/39] loss 4.4371 accuracy 1.56%\n",
            "[epoch 0][36/39] loss 4.4722 accuracy 0.78%\n",
            "[epoch 0][39/39] loss 4.1007 accuracy 25.00%\n",
            "\n",
            "epoch 1 learning rate 0.090250\n",
            "\n",
            "[epoch 1][0/39] loss 4.3485 accuracy 3.91%\n",
            "[epoch 1][3/39] loss 4.3648 accuracy 3.12%\n",
            "[epoch 1][6/39] loss 4.3658 accuracy 4.69%\n",
            "[epoch 1][9/39] loss 4.3276 accuracy 3.12%\n",
            "[epoch 1][12/39] loss 4.3792 accuracy 3.91%\n",
            "[epoch 1][15/39] loss 4.2496 accuracy 5.47%\n",
            "[epoch 1][18/39] loss 4.3291 accuracy 3.91%\n",
            "[epoch 1][21/39] loss 4.2954 accuracy 4.69%\n",
            "[epoch 1][24/39] loss 4.1419 accuracy 3.91%\n",
            "[epoch 1][27/39] loss 4.2598 accuracy 3.12%\n",
            "[epoch 1][30/39] loss 4.2241 accuracy 6.25%\n",
            "[epoch 1][33/39] loss 4.1665 accuracy 4.69%\n",
            "[epoch 1][36/39] loss 4.2141 accuracy 6.25%\n",
            "[epoch 1][39/39] loss 3.5539 accuracy 37.50%\n",
            "\n",
            "epoch 2 learning rate 0.085737\n",
            "\n",
            "[epoch 2][0/39] loss 4.3571 accuracy 3.91%\n",
            "[epoch 2][3/39] loss 4.4048 accuracy 1.56%\n",
            "[epoch 2][6/39] loss 4.2961 accuracy 3.12%\n",
            "[epoch 2][9/39] loss 4.1512 accuracy 3.91%\n",
            "[epoch 2][12/39] loss 4.2981 accuracy 5.47%\n",
            "[epoch 2][15/39] loss 4.2184 accuracy 3.12%\n",
            "[epoch 2][18/39] loss 4.2861 accuracy 3.12%\n",
            "[epoch 2][21/39] loss 4.3188 accuracy 2.34%\n",
            "[epoch 2][24/39] loss 4.2606 accuracy 2.34%\n",
            "[epoch 2][27/39] loss 4.3219 accuracy 7.03%\n",
            "[epoch 2][30/39] loss 4.3627 accuracy 2.34%\n",
            "[epoch 2][33/39] loss 4.1734 accuracy 6.25%\n",
            "[epoch 2][36/39] loss 4.2439 accuracy 3.91%\n",
            "[epoch 2][39/39] loss 3.2509 accuracy 37.50%\n",
            "\n",
            "epoch 3 learning rate 0.081451\n",
            "\n",
            "[epoch 3][0/39] loss 4.1084 accuracy 4.69%\n",
            "[epoch 3][3/39] loss 4.1792 accuracy 7.03%\n",
            "[epoch 3][6/39] loss 4.2563 accuracy 4.69%\n",
            "[epoch 3][9/39] loss 4.0142 accuracy 8.59%\n",
            "[epoch 3][12/39] loss 4.2445 accuracy 9.38%\n",
            "[epoch 3][15/39] loss 4.1300 accuracy 6.25%\n",
            "[epoch 3][18/39] loss 4.2194 accuracy 3.12%\n",
            "[epoch 3][21/39] loss 4.1469 accuracy 5.47%\n",
            "[epoch 3][24/39] loss 4.1194 accuracy 6.25%\n",
            "[epoch 3][27/39] loss 4.2142 accuracy 5.47%\n",
            "[epoch 3][30/39] loss 4.1921 accuracy 6.25%\n",
            "[epoch 3][33/39] loss 4.0499 accuracy 8.59%\n",
            "[epoch 3][36/39] loss 4.0893 accuracy 5.47%\n",
            "[epoch 3][39/39] loss 2.8729 accuracy 37.50%\n",
            "\n",
            "epoch 4 learning rate 0.077378\n",
            "\n",
            "[epoch 4][0/39] loss 4.0848 accuracy 7.03%\n",
            "[epoch 4][3/39] loss 4.0406 accuracy 3.91%\n",
            "[epoch 4][6/39] loss 4.1303 accuracy 5.47%\n",
            "[epoch 4][9/39] loss 3.9860 accuracy 7.81%\n",
            "[epoch 4][12/39] loss 4.1009 accuracy 8.59%\n",
            "[epoch 4][15/39] loss 4.0596 accuracy 7.03%\n",
            "[epoch 4][18/39] loss 4.1874 accuracy 8.59%\n",
            "[epoch 4][21/39] loss 3.9845 accuracy 6.25%\n",
            "[epoch 4][24/39] loss 4.0068 accuracy 7.81%\n",
            "[epoch 4][27/39] loss 4.0776 accuracy 7.81%\n",
            "[epoch 4][30/39] loss 4.0159 accuracy 3.91%\n",
            "[epoch 4][33/39] loss 3.8690 accuracy 11.72%\n",
            "[epoch 4][36/39] loss 4.0134 accuracy 7.81%\n",
            "[epoch 4][39/39] loss 2.4068 accuracy 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "testset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=False, download=False, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0, worker_init_fn = lambda id: np.random.seed(id))\n",
        "model = AttnVGG_before(im_size=32, num_classes=100, attention=True, normalize_attn=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load('/content/net.pth', map_location=torch.device(\"cuda\")))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "        images_test, labels_test = data\n",
        "        images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
        "        pred_test, __, __, __ = model(images_test)\n",
        "        predict = torch.argmax(pred_test, 1)\n",
        "        total += labels_test.size(0)\n",
        "        correct += torch.eq(predict, labels_test).sum().double().item()\n",
        "    print(\"\\nAccuracy on test data: %.2f%%\\n\" % (100*correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbvpZ-8A9vbF",
        "outputId": "11ed406f-34cd-454d-9f26-c2d08481e54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy on test data: 6.03%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}